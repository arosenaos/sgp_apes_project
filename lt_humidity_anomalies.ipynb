{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.ticker as ticker\n",
    "import cartopy.crs as ccrs\n",
    "from scipy.interpolate import griddata\n",
    "from matplotlib.patches import Rectangle\n",
    "import cartopy.feature as cfeature\n",
    "import glob\n",
    "from datetime import date\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from haversine import haversine, Unit\n",
    "import metpy\n",
    "import metpy.constants as mpconst\n",
    "import metpy.units as units\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import bisect\n",
    "import numpy as np\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import pint\n",
    "import xarray as xr\n",
    "import netCDF4 as nc\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import load\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "import pytz\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from collections import Counter\n",
    "import pymannkendall as mk\n",
    "from pydoc import help\n",
    "import warnings\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Set display options to show all rows and columns in dataframe\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "# np.set_printoptions(threshold=np.inf)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('soundings_fig2&3.pdkl', 'rb') as file:\n",
    "    \n",
    "    soundings = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78, 27)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soundings[(soundings['APE']==True) & (soundings['wet_coupling']==True)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"../../data/rong4/Data/ERA5/3hourly/quvw_US/specific_humidity\"\n",
    "\n",
    "def extract_year_and_month(filename):\n",
    "    # Assuming the year is always after the \".specific_humidity.\" part\n",
    "    \n",
    "    parts = filename.split('.')\n",
    "    \n",
    "    for part in parts:\n",
    "        \n",
    "        if part.isdigit() and len(part) == 6:  # Check if it's a year\n",
    "            \n",
    "            year = int(part[0:4])\n",
    "            \n",
    "            month = int(part[4:6])\n",
    "            \n",
    "            return year, month\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "files = [\n",
    "    \n",
    "    os.path.join(directory_path, file)\n",
    "    \n",
    "    for file in os.listdir(directory_path)\n",
    "    \n",
    "    if file.endswith(\".nc\")\n",
    "    \n",
    "    and 2001 <= extract_year_and_month(file)[0] <= 2018\n",
    "    \n",
    "    and 5 <= extract_year_and_month(file)[1] <= 9\n",
    "]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for ds in sorted(files):\n",
    "    \n",
    "    ds = xr.open_dataset(ds)\n",
    "    \n",
    "    dfs.append(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_q_data = []\n",
    "\n",
    "level_min = 600\n",
    "\n",
    "level_max = 800\n",
    "\n",
    "_12lst = 18\n",
    "\n",
    "_15lst = 21\n",
    "\n",
    "for df in dfs:\n",
    "    \n",
    "    # Select the levels within the desired range\n",
    "    df_level = df.sel(level=slice(level_min, level_max))\n",
    "    \n",
    "    # Filter for the specific time\n",
    "    df_filtered = df_level.sel(time=df_level['time'].dt.hour == _12lst)\n",
    "    \n",
    "    # Extract time, latitude, and longitude data\n",
    "    times = df_filtered['time'].data\n",
    "    \n",
    "    # Extract q values, which will now include multiple levels\n",
    "    q_values = df_filtered['q'].data\n",
    "    \n",
    "    for t_idx in range(q_values.shape[0]):\n",
    "        \n",
    "        time_value = times[t_idx]\n",
    "        \n",
    "        #q_values[t_idx] has shape (6, 101, 241), code below calculates elementwise (per lat/lon) average of q in LT layer \n",
    "    \n",
    "        q_per_time = np.mean(q_values[t_idx], axis=0)\n",
    "        \n",
    "        all_q_data.append((time_value, q_per_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean_lt_q is the average 800-600 hPa LT humidity\n",
    "qdf = pd.DataFrame(all_q_data, columns=['utc','ltq'])\n",
    "\n",
    "#converting to g/kg\n",
    "qdf['ltq'] = qdf['ltq']*1000\n",
    "\n",
    "#converting to LST to avoid confusion\n",
    "qdf['lst'] = qdf['utc'] - timedelta(hours=6)\n",
    "\n",
    "qdf['lst'] = pd.to_datetime(qdf['lst'])\n",
    "\n",
    "qdf['date'] = qdf['lst'].dt.date\n",
    "\n",
    "qdf.drop(columns=['utc'], inplace=True)\n",
    "\n",
    "qdf = qdf.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lons = dfs[0]['longitude'].data\n",
    "\n",
    "lats = dfs[0]['latitude'].data\n",
    "\n",
    "lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "\n",
    "# Flatten the latitude and longitude grids\n",
    "lat_flat = lat_grid.ravel()\n",
    "\n",
    "lon_flat = lon_grid.ravel()\n",
    "\n",
    "def reshape_df(df):\n",
    "    # Explode the 'humidity' column to have each value in a separate row\n",
    "    exploded_df = df.explode('ltq').reset_index(drop=True)\n",
    "    \n",
    "    # Now, explode each array in 'humidity' into separate rows (for each lat-lon point)\n",
    "    exploded_df['ltq'] = exploded_df['ltq'].apply(lambda x: x.ravel())\n",
    "\n",
    "    # Expand the DataFrame so that each row corresponds to a lat-lon point\n",
    "    expanded_df = exploded_df.explode('ltq').reset_index(drop=True)\n",
    "\n",
    "    # Assign lat and lon values\n",
    "    expanded_df['latitude'] = np.tile(lat_flat, len(df))\n",
    "    \n",
    "    expanded_df['longitude'] = np.tile(lon_flat, len(df))\n",
    "    \n",
    "    return expanded_df\n",
    "\n",
    "# Assuming 'df' is your DataFrame with a column named 'humidity'\n",
    "qdf_reshaped = reshape_df(qdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdf_reshaped['day'] = qdf_reshaped['lst'].dt.date\n",
    "\n",
    "qdf_reshaped['month_day'] = qdf_reshaped['day'].apply(lambda x: x.strftime('%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#climatology calculation\n",
    "\n",
    "clim_q = qdf_reshaped.groupby(['month_day', 'latitude', 'longitude']).agg({\n",
    "    \n",
    "    'ltq': ['mean', 'std', 'count'] \n",
    "        \n",
    "}).reset_index()\n",
    "\n",
    "clim_q.set_index('month_day', inplace=True)\n",
    "\n",
    "qdf_reshaped.set_index('month_day', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardized anomaly calculation\n",
    "\n",
    "clim_q.columns = ['_'.join(col).strip() if col[1] else col[0] for col in clim_q.columns.values]\n",
    "\n",
    "anom_q = pd.merge(qdf_reshaped, clim_q, on=['month_day', 'latitude', 'longitude'], how='inner')\n",
    "\n",
    "anom_q['q_stnd_anom'] = (anom_q['ltq'] - anom_q['ltq_mean'])/anom_q['ltq_std']\n",
    "\n",
    "anom_q = anom_q.set_index('day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_anom_sound = pd.merge(anom_q, soundings, left_index=True, right_index=True, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltq_dryapes = q_anom_sound[(q_anom_sound['dry_coupling']==True) & (q_anom_sound['APE']==True)]\n",
    "\n",
    "ltq_wetapes = q_anom_sound[(q_anom_sound['wet_coupling']==True) & (q_anom_sound['APE']==True)]\n",
    "\n",
    "ltq_dryapes.index.name = 'day'\n",
    "\n",
    "ltq_wetapes.index.name = 'day'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing shape so as easier to plot\n",
    "\n",
    "ltq_dryapes = ltq_dryapes.drop(columns=['day'])\n",
    "\n",
    "ltq_dryapes = ltq_dryapes.pivot_table(index='day', columns=['latitude', 'longitude'], values=['q_stnd_anom'])\n",
    "\n",
    "ltq_dryapes.reset_index(inplace=True)\n",
    "\n",
    "#flattening the MultiIndex column names\n",
    "\n",
    "ltq_dryapes.columns = [''.join(map(str, col)) for col in ltq_dryapes.columns]\n",
    "\n",
    "ltq_dryapes = ltq_dryapes.set_index('day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing shape so as easier to plot\n",
    "\n",
    "ltq_wetapes = ltq_wetapes.drop(columns=['day'])\n",
    "\n",
    "ltq_wetapes = ltq_wetapes.pivot_table(index='day', columns=['latitude', 'longitude'], values=['q_stnd_anom'])\n",
    "\n",
    "ltq_wetapes.reset_index(inplace=True)\n",
    "\n",
    "#flattening the MultiIndex column names\n",
    "\n",
    "ltq_wetapes.columns = [''.join(map(str, col)) for col in ltq_wetapes.columns]\n",
    "\n",
    "ltq_wetapes = ltq_wetapes.set_index('day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 78)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ltq_dryapes.shape[0], ltq_wetapes.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ltq_wetapes.pdkl', 'wb') as f:\n",
    "    \n",
    "    pickle.dump(ltq_wetapes, f)\n",
    "    \n",
    "with open('ltq_dryapes.pdkl', 'wb') as f:\n",
    "    \n",
    "    pickle.dump(ltq_dryapes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "annieenv",
   "language": "python",
   "name": "annieenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
