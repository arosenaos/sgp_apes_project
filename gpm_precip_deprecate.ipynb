{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gpm.nasa.gov/sites/default/files/2023-07/IMERG_TechnicalDocumentation_final_230713.pdf (technical documentation on GPM)\n",
    "\n",
    "In IMERG, the half-hourly precipitation measurement is based on a single microwave snapshot (or interpolated data if no microwave data are available). The precipitation rate in mm/hr is assumed to be valid for the entire half-hour period (mm/hr)\n",
    "\n",
    "to do: \n",
    "- potentially adjust threshold for qi index\n",
    "- nan logic for handling nans in morning, afternoon or evening precip and how to calculate those sums\n",
    "\n",
    "\n",
    "Gaoyun APE def: \n",
    "\n",
    "morning (0600-1300 LST), afternoon (1400-2000 LST), and evening (2100-2400 LST) precipitation events in our analysis. Afternoon precipitation events (APEs) are identified as daily samples that meet the following two criteria: 1) daily precipitation peaks during the afternoon hours defined above; and 2) the afternoon precipitation is at least twice as large as the morning precipitation, and also greater than the evening precipitation (filter out organized precipitation).\n",
    "\n",
    "APE def from other paper: \n",
    "\n",
    "If on a given day the 3°x3° domain receives less than 0.004 mm precipitation during the morning hours (defined as 0700 –1100 CST, i.e. 0.001 mm hr-1 average), greater than 0.004 mm during the “afternoon” hours (defined as 1100 – 2300 CST), and the daily precipitation peak occurs during one of the “afternoon” hours, this day is classified as APE. https://agupubs.onlinelibrary.wiley.com/action/downloadSupplement?doi=10.1029%2F2018GL078598&file=grl57813-sup-0001-2018GL078598_SI.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "import h5py\n",
    "import gc\n",
    "import datetime\n",
    "from urllib.parse import urljoin\n",
    "import pytz\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7344\n"
     ]
    }
   ],
   "source": [
    "#reading in HDF5 files\n",
    "folder_path = '/data/rong3/annie/gpm/2000'\n",
    "\n",
    "gpm_files = sorted(glob.glob(os.path.join(folder_path, '*.HDF5')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parallel processed\n",
    "\n",
    "base_date = datetime.datetime(1980, 1, 6, 0, 0, 0)\n",
    "\n",
    "lat_start = 30.05\n",
    "\n",
    "lat_end = 39.05\n",
    "\n",
    "lon_start = -105.05\n",
    "\n",
    "lon_end = -95.05\n",
    "\n",
    "def convert_to_cst(seconds_since_base):\n",
    "    \n",
    "    if isinstance(seconds_since_base, (np.int32, np.int64)):\n",
    "        \n",
    "        seconds_since_base = int(seconds_since_base)\n",
    "    \n",
    "    time_delta = timedelta(seconds=seconds_since_base)\n",
    "    \n",
    "    # Calculate UTC time\n",
    "    utc_time = base_date + time_delta\n",
    "    \n",
    "    # Convert UTC time to CST (subtract 6 hours for CST)\n",
    "    cst_time = utc_time - timedelta(hours=6)\n",
    "    \n",
    "    # Remove timezone information to make it naive\n",
    "    naive_cst_time = cst_time.replace(tzinfo=None)\n",
    "\n",
    "    return naive_cst_time\n",
    "\n",
    "def process_file(file):\n",
    "    \n",
    "    threshold_nans = 1510 #10% of total in lat/lon range\n",
    "    \n",
    "    threshold_qi = 0.4 #above 0.4 falls into \"fair\" and \"good\" based on documentation\n",
    "    \n",
    "    if os.path.isfile(file) and os.path.getsize(file) > 0:\n",
    "        \n",
    "        with h5py.File(file, 'r', locking=False) as f:\n",
    "            \n",
    "            # Geographical filtering\n",
    "            \n",
    "            lon = f['Grid/lon'][:]\n",
    "            \n",
    "            lat = f['Grid/lat'][:]\n",
    "\n",
    "            lon_mask = (lon >= lon_start) & (lon <= lon_end)\n",
    "            \n",
    "            lat_mask = (lat >= lat_start) & (lat <= lat_end)\n",
    "\n",
    "            lon_indices = np.where(lon_mask)[0]\n",
    "            \n",
    "            lat_indices = np.where(lat_mask)[0]\n",
    "\n",
    "            # Time conversion\n",
    "            time = f['Grid/time'][:][0]\n",
    "            \n",
    "            lst = convert_to_cst(time)\n",
    "\n",
    "            # Precipitation filtering by geography\n",
    "            precip = xr.DataArray(f['Grid/precipitation'][:][0], dims=['lon', 'lat'])\n",
    "            \n",
    "            psub = precip.isel(lon=lon_indices, lat=lat_indices)\n",
    "            \n",
    "            psub = psub.where(psub >= 0, np.nan)  # Replace all negative values with NaNs\n",
    "\n",
    "            # Precipitation filtering with quality index\n",
    "            precip_qi = xr.DataArray(f['Grid/precipitationQualityIndex'][:][0], dims=['lon', 'lat'])\n",
    "            \n",
    "            psub_qi = precip_qi.isel(lon=lon_indices, lat=lat_indices)\n",
    "            \n",
    "            qi_mask = psub_qi > threshold_qi\n",
    "            \n",
    "            qi_psub = psub.where(qi_mask, np.nan)\n",
    "\n",
    "            nans = np.sum(np.isnan(qi_psub)).item()\n",
    "\n",
    "            if nans < threshold_nans:\n",
    "                \n",
    "                sgp_precip_average = np.nanmean(qi_psub)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                sgp_precip_average = np.nan\n",
    "\n",
    "            return lst, sgp_precip_average\n",
    "\n",
    "    else:\n",
    "        \n",
    "        print(f\"Not a file: {file}\")\n",
    "        \n",
    "        return None, None\n",
    "\n",
    "def read_and_filter_data_parallel(files):\n",
    "    \n",
    "    num_files = len(files)\n",
    "    \n",
    "    lst_array = []\n",
    "    \n",
    "    precip_means = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=20) as executor: #max_workers means # of threads at once\n",
    "        \n",
    "        future_to_file = {executor.submit(process_file, file): file for file in files}\n",
    "        \n",
    "        for future in as_completed(future_to_file):\n",
    "            \n",
    "            lst, sgp_precip_average = future.result()\n",
    "            \n",
    "            if lst is not None:\n",
    "                \n",
    "                lst_array.append(lst)\n",
    "                \n",
    "                precip_means.append(sgp_precip_average)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    lst_array = np.array(lst_array, dtype='datetime64[ns]')\n",
    "    \n",
    "    precip_means = np.array(precip_means, dtype=float)\n",
    "\n",
    "    return lst_array, precip_means\n",
    "\n",
    "\n",
    "lst_array, precip_means = read_and_filter_data_parallel(gpm_files)\n",
    "\n",
    "df = pd.DataFrame({'LST': lst_array, 'mm_per_hr': precip_means}).sort_values(by='LST')\n",
    "\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APE classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert from mm/hr to total mm, mm/hr*0.5hr = mm \n",
    "df['total_mm'] = df['mm_per_hr']*0.5\n",
    "\n",
    "#calculating total mm for morning, afternoon and evening based on gaoyun's definition \n",
    "df['hr_min'] = pd.to_datetime(df['LST']).dt.strftime('%H:%M')\n",
    "\n",
    "df['date'] = pd.to_datetime(df['LST']).dt.date\n",
    "\n",
    "def classify_time_of_day(time):\n",
    "    \n",
    "    if \"06:00\" <= time < \"14:00\": #8 hours x 2 = 16 total observations\n",
    "        \n",
    "        return 'morning'\n",
    "    \n",
    "    elif \"14:00\" <= time < \"21:00\": #7 hours x 2 = 14 total observations\n",
    "        \n",
    "        return 'afternoon'\n",
    "    \n",
    "    elif \"21:00\" <= time <= \"23:30\": #3 hours x 2 = 6 total observations\n",
    "        \n",
    "        return 'evening'\n",
    "\n",
    "# # Apply the function to create a new column\n",
    "df['time_of_day'] = df['hr_min'].apply(classify_time_of_day)\n",
    "\n",
    "df = df[pd.notna(df[\"time_of_day\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nans adjustment\n",
    "na_counts = df.groupby(['date', 'time_of_day'])['total_mm'].apply(lambda x: x.isna().sum())\n",
    "\n",
    "tod_means = df.groupby(['date', 'time_of_day'])['total_mm'].mean()\n",
    "\n",
    "total_counts = df.groupby(['date', 'time_of_day'])['total_mm'].count() + na_counts\n",
    "\n",
    "proportions_na = na_counts / total_counts\n",
    "\n",
    "proportions_df = proportions_na.reset_index(name='proportions_na')\n",
    "\n",
    "tod_means_df = tod_means.reset_index(name='tod_means')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(proportions_df, on=['date', 'time_of_day'], how='left')\n",
    "\n",
    "df = df.merge(tod_means_df, on=['date', 'time_of_day'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nans_adjustment(row):\n",
    "    \n",
    "    if row['proportions_na'] >= 0.5:\n",
    "        \n",
    "        return np.nan\n",
    "    \n",
    "    elif row['proportions_na'] < 0.5 and not pd.isna(row['total_mm']):\n",
    "        \n",
    "        return row['total_mm']\n",
    "    \n",
    "    elif row['proportions_na'] < 0.5 and pd.isna(row['total_mm']):\n",
    "        \n",
    "        return row['tod_means']\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return np.nan\n",
    "\n",
    "# Apply the function and create a new column 'total_mm_clean'\n",
    "df['total_mm_clean'] = df.apply(nans_adjustment, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_sum(values):\n",
    "    \n",
    "    if pd.isna(values).any():\n",
    "        \n",
    "        return np.nan\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#condition 1 -> afternoon is 2x morning and also greater than evening\n",
    "\n",
    "sums = df.pivot_table(index='date', columns='time_of_day', values='total_mm_clean', aggfunc=custom_sum)\n",
    "\n",
    "sums = sums.dropna(how='any')\n",
    "\n",
    "sums['cond1'] = (sums['afternoon'] >= 2 * sums['morning']) & (sums['afternoon'] > sums['evening'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#condition 2\n",
    "\n",
    "peak = df.loc[df.groupby('date')['total_mm'].idxmax()][['date', 'hr_min']]\n",
    "\n",
    "peak['cond2'] = peak['hr_min'].apply(classify_time_of_day)\n",
    "\n",
    "peak = peak.reset_index(drop=True)\n",
    "\n",
    "ape_conditions = pd.merge(sums, peak, on='date', how='inner')\n",
    "\n",
    "ape_conditions['APE'] = (ape_conditions['cond1'] == True) & (ape_conditions['cond2'] == 'afternoon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>evening</th>\n",
       "      <th>morning</th>\n",
       "      <th>cond1</th>\n",
       "      <th>hr_min</th>\n",
       "      <th>cond2</th>\n",
       "      <th>APE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-05-02</td>\n",
       "      <td>1.209279</td>\n",
       "      <td>1.028254</td>\n",
       "      <td>0.020153</td>\n",
       "      <td>True</td>\n",
       "      <td>22:30</td>\n",
       "      <td>evening</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-05-03</td>\n",
       "      <td>0.952842</td>\n",
       "      <td>0.768568</td>\n",
       "      <td>0.769582</td>\n",
       "      <td>False</td>\n",
       "      <td>22:30</td>\n",
       "      <td>evening</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-05-04</td>\n",
       "      <td>3.988190</td>\n",
       "      <td>1.891001</td>\n",
       "      <td>3.610083</td>\n",
       "      <td>False</td>\n",
       "      <td>18:30</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-05-07</td>\n",
       "      <td>0.163817</td>\n",
       "      <td>0.016172</td>\n",
       "      <td>0.450646</td>\n",
       "      <td>False</td>\n",
       "      <td>06:00</td>\n",
       "      <td>morning</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-05-08</td>\n",
       "      <td>0.225474</td>\n",
       "      <td>0.056071</td>\n",
       "      <td>0.051460</td>\n",
       "      <td>True</td>\n",
       "      <td>16:30</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2001-05-11</td>\n",
       "      <td>1.538211</td>\n",
       "      <td>1.208882</td>\n",
       "      <td>0.275501</td>\n",
       "      <td>True</td>\n",
       "      <td>20:30</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2001-05-12</td>\n",
       "      <td>0.430330</td>\n",
       "      <td>0.064929</td>\n",
       "      <td>0.244665</td>\n",
       "      <td>False</td>\n",
       "      <td>19:00</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2001-05-13</td>\n",
       "      <td>0.117809</td>\n",
       "      <td>0.049994</td>\n",
       "      <td>0.008302</td>\n",
       "      <td>True</td>\n",
       "      <td>19:30</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2001-05-14</td>\n",
       "      <td>0.262484</td>\n",
       "      <td>0.019342</td>\n",
       "      <td>0.022757</td>\n",
       "      <td>True</td>\n",
       "      <td>18:30</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2001-05-15</td>\n",
       "      <td>0.006363</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.000884</td>\n",
       "      <td>True</td>\n",
       "      <td>19:00</td>\n",
       "      <td>afternoon</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2001-05-17</td>\n",
       "      <td>2.914807</td>\n",
       "      <td>1.866528</td>\n",
       "      <td>0.378401</td>\n",
       "      <td>True</td>\n",
       "      <td>23:00</td>\n",
       "      <td>evening</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  afternoon   evening   morning  cond1 hr_min      cond2    APE\n",
       "0   2001-05-02   1.209279  1.028254  0.020153   True  22:30    evening  False\n",
       "1   2001-05-03   0.952842  0.768568  0.769582  False  22:30    evening  False\n",
       "2   2001-05-04   3.988190  1.891001  3.610083  False  18:30  afternoon  False\n",
       "3   2001-05-07   0.163817  0.016172  0.450646  False  06:00    morning  False\n",
       "4   2001-05-08   0.225474  0.056071  0.051460   True  16:30  afternoon   True\n",
       "5   2001-05-11   1.538211  1.208882  0.275501   True  20:30  afternoon   True\n",
       "6   2001-05-12   0.430330  0.064929  0.244665  False  19:00  afternoon  False\n",
       "7   2001-05-13   0.117809  0.049994  0.008302   True  19:30  afternoon   True\n",
       "8   2001-05-14   0.262484  0.019342  0.022757   True  18:30  afternoon   True\n",
       "9   2001-05-15   0.006363  0.002105  0.000884   True  19:00  afternoon   True\n",
       "10  2001-05-17   2.914807  1.866528  0.378401   True  23:00    evening  False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ape_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.45454545454545"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total count of APEs in set\n",
    "\n",
    "(np.sum(ape_conditions['APE'] == True)/ape_conditions.shape[0])*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>evening</th>\n",
       "      <th>morning</th>\n",
       "      <th>cond1</th>\n",
       "      <th>hr_min</th>\n",
       "      <th>cond2</th>\n",
       "      <th>APE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [date, afternoon, evening, morning, cond1, hr_min, cond2, APE]\n",
       "Index: []"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ape_conditions[ape_conditions['APE']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_76032/1044116339.py:8: FutureWarning: The provided callable <function nanmean at 0x7f1c0e2a3af0> is currently using SeriesGroupBy.mean. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"mean\" instead.\n",
      "  tod_means_nans = df.groupby(['date', 'time_of_day'])['total_mm'].agg(np.nanmean)\n"
     ]
    }
   ],
   "source": [
    "# #to do: fix this, if less than half of the values are nans for each time and time of day, replace nans with average\n",
    "# #value, otherwise leave as nans and let pivot table (with fill_value=np.nan) keep as nan\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # proportions_df.set_index(['date', 'time_of_day'], inplace=True)\n",
    "\n",
    "# def fill_na(row):\n",
    "#     key = (row['date'], row['time_of_day'])\n",
    "    \n",
    "#     # Set the index and lookup the proportion\n",
    "#     proportion = proportions_df.set_index(['date', 'time_of_day']).at[key, 'proportions_na'] if key in proportions_df.set_index(['date', 'time_of_day']).index else np.nan\n",
    "    \n",
    "#     if proportion > 0.5:\n",
    "#         return row['total_mm']  # Leave NaNs as NaNs\n",
    "#     else:\n",
    "#         # Calculate the mean of total_mm for the same date and time_of_day\n",
    "#         avg_value = df[(df['date'] == row['date']) & \n",
    "#                        (df['time_of_day'] == row['time_of_day'])]['total_mm'].mean()\n",
    "        \n",
    "#         return avg_value if pd.isna(row['total_mm']) else row['total_mm']\n",
    "\n",
    "# # Apply the filling function\n",
    "# df['total_mm_clean'] = df.apply(fill_na, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not parallel processed \n",
    "\n",
    "\n",
    "def read_and_filter_data(files):\n",
    "    \n",
    "    threshold_nans = 1510 #10% of total in lat/lon range\n",
    "    \n",
    "    threshold_qi = 0.4 #above 0.4 falls into \"fair\" and \"good\" based on documentation, below \"questionable\"\n",
    "    \n",
    "    num_files = 140000 #for 153 (warm season days in every year) * 48 (half-hourly measurements) * 19 (total years) = 139536 files\n",
    "    \n",
    "    lst_array = np.empty(num_files, dtype='datetime64[ns]')\n",
    "    \n",
    "    precip_means = np.empty(num_files, dtype=float)\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    for file in files:\n",
    "        \n",
    "        if os.path.isfile(file) and os.path.getsize(file) > 0:\n",
    "            \n",
    "            with h5py.File(file, 'r', locking=False) as f:\n",
    "                \n",
    "                #geographical filtering \n",
    "                lon = f['Grid/lon'][:]\n",
    "                \n",
    "                lat = f['Grid/lat'][:]\n",
    "\n",
    "                lon_mask = (lon >= lon_start) & (lon <= lon_end)\n",
    "                \n",
    "                lat_mask = (lat >= lat_start) & (lat <= lat_end)\n",
    "\n",
    "                lon_indices = np.where(lon_mask)[0]\n",
    "                \n",
    "                lat_indices = np.where(lat_mask)[0]\n",
    "                \n",
    "                #time conversion\n",
    "                time = f['Grid/time'][:][0]\n",
    "                \n",
    "                lst = convert_to_cst(time)\n",
    "                \n",
    "                #precipitation filtering by geography\n",
    "                precip = xr.DataArray(f['Grid/precipitation'][:][0], dims=['lon', 'lat'])\n",
    "                \n",
    "                psub = precip.isel(lon=lon_indices, lat=lat_indices)\n",
    "                \n",
    "                psub = psub.where(psub >=0, np.nan) #replace all negative values (which includes -9999.9) values with nans\n",
    "                \n",
    "                #precipitation filtering with quality index\n",
    "                precip_qi = xr.DataArray(f['Grid/precipitationQualityIndex'][:][0], dims=['lon', 'lat'])\n",
    "                \n",
    "                psub_qi = precip_qi.isel(lon=lon_indices, lat=lat_indices)\n",
    "                \n",
    "                qi_check = np.sum(psub_qi.where(psub_qi > 0.4))\n",
    "                \n",
    "                qi_total = psub_qi.shape[0]*psub_qi.shape[1]\n",
    "                                  \n",
    "                qi_mask = psub_qi > threshold_qi\n",
    "\n",
    "                qi_psub = psub.where(qi_mask, np.nan)\n",
    "                \n",
    "                nans = np.sum(np.isnan(qi_psub)).item()\n",
    "                \n",
    "                lst_array[count] = lst\n",
    "\n",
    "                if nans < threshold_nans: \n",
    "                    \n",
    "                    sgp_precip_average = np.nanmean(qi_psub)\n",
    "                \n",
    "                else: \n",
    "                    \n",
    "                    sgp_precip_average = np.nan\n",
    "    \n",
    "                    \n",
    "                precip_means[count] = sgp_precip_average\n",
    "                \n",
    "                count += 1\n",
    "                    \n",
    "        else:\n",
    "            print(f\"Not a file: {file}\")\n",
    "    \n",
    "    return lst_array[:count], precip_means[:count]\n",
    "\n",
    "lst_array, precip_means = read_and_filter_data(gpm_files)\n",
    "\n",
    "non_par_df = pd.DataFrame({'LST': lst_array, 'mean precip': precip_means})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = gpm_small[0]\n",
    "\n",
    "# def print_item(name, obj):\n",
    "    \n",
    "#     if isinstance(obj, h5py.Group):\n",
    "        \n",
    "#         print(f\"Group: {name}\")\n",
    "    \n",
    "#     elif isinstance(obj, h5py.Dataset):\n",
    "        \n",
    "#         print(f\"Dataset: {name}\")\n",
    "#         # Print attributes for this dataset\n",
    "        \n",
    "#         print(\"  Attributes:\")\n",
    "        \n",
    "#         for attr_name, attr_value in obj.attrs.items():\n",
    "            \n",
    "#             print(f\"    {attr_name}: {attr_value}\")\n",
    "\n",
    "# with h5py.File(file_path, 'r', locking=False) as f:\n",
    "    \n",
    "#     f.visititems(print_item)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-179.95   , -179.85   , -179.75   , ...,  179.75   ,  179.84999,\n",
       "        179.95   ], dtype=float32)"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = gpm_files[0]\n",
    "        \n",
    "if os.path.isfile(file) and os.path.getsize(file) > 0:\n",
    "\n",
    "    with h5py.File(file, 'r', locking=False) as f:\n",
    "        \n",
    "        lon = f['Grid/lon'][:]\n",
    "        \n",
    "        lat = f['Grid/lat'][:]\n",
    "\n",
    "lon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "annieenv",
   "language": "python",
   "name": "annieenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
